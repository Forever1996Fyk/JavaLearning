## HashMap相关面试题

之前学习了`HashMap`, 趁着还没忘, 从网上收集一些`HashMap`相关的面试题。

### 1. HashMap的数据结构是什么?

`HashMap`是基于哈希表的结构实现的, 也就是数组+链表。结合了数组和链表的优点。在JDK1.8之后, 还加入了红黑树, 当链表的长度超过8时, 链表会转换为红黑树。

### 2. HashMap的工作原理是什么?

`HashMap`底层是hash数组和单向链表实现, 数组中的每个元素都封装成Node节点, Node实现了`Map.Entry`接口, `HashMap`通过put & get方法存储和获取。

**存储对象时, 将key, value 键值对通过put方法传入:**

1. 调用`hash(key)`方法计算key的hash值, 然后结合数组长度, 计算数组下标; 具体算法就是把hash值与数组长度减一进行按位与运算, `hash & (length - 1)`;

2. 调整数组大小, 当容器中的元素个数大于`capacity * loadfactor`时, 容器会进行扩容调用`resize`方法, 扩容为原来的2倍;

3. 判断key的hash值在`HashMap`中是否存在:

    - 如果不存在则直接插入到数组下标位置;

    - 如果存在, 再判断key值是否相等, 也就是调用equals方法。 如果不存在, 则直接覆盖原value的值, 否则说明发生了哈希碰撞;

    - 发生哈希碰撞, 再判断相应位置的元素是否是`TreeNode`类型, 如果不是, 则表示当前是链表结构, 那就直接在链表的尾部插入元素, 如果链表的长度大于8时, 就将链表的结构树化; 如果当前位置元素类型是`TreeNode`, 则说明当前数据结构是红黑树, 那就将元素插入到红黑树中。

**获取对象是, 将key值传给get()方法:**

1. 调用hash(key)方法计算key的hash值, 然后再次进行计算数组下标;
2. 获取当前位置的Node元素, 通过equals方法判断当前元素的key值是否相同:

    - 如果相同则直接返回value值;

    - 如果不相同, 则判断Node的next节点是否为空;  如果不为空, 再判断Node类型是否是`TreeNode`, 如果是, 则遍历红黑树, 查找元素; 否则遍历链表

通过hashCode可以获取到哈希表的下标, 存储位置; equals比较两个key是否相等

### 3. 当两个对象的hashcode相同会发生什么?

因为对象的hashCode相同, 不一定就是相等的, 所以两个对象所在的数组下标相同, 可能会发生碰撞。

### 4. HashMap中hash函数的实现是怎样的? 为什么要这样实现?

在JDK1.8中, 是通过hashcode的高16位与hashcode的低16位进行异或运算, `(h = key.hashCode()) ^ (h >>> 16)`。主要从速度和减少hash碰撞的问题考虑。

### 5. 为什么要用异或运算符?

使用异或运算, 主要是保证对象的hashCode的32位, 只要有一位发生改变, 整个hash()返回值就会改变, 尽可能的减少碰撞。

### 6. HashMap的table容量如何确定? loadFactor是什么? 该容量如何变化? 这种变化会带来什么问题>

1. hash表的大小是由capacity参数确定的, 默认大小是16, 也可以通过构造函数传入, 最大限制是1<<30;

2. `loadFactor`是装载因子, 主要目的就是用来确认table数组是否需要动态扩展, 默认值是0.75。

    > 比如table数组大小为16, 装载因子为0.75时, threshold就是12。当table的实际大小超过12时, table就需要进行扩容

3. 扩容时调用`resize`方法, 将table长度变为原来的2倍;

4. 当数据量很大的情况下, 由于在扩容后, 还需要对原hash表进行遍历, 重新分配元素的位置; 这就会导致性能的损失。

### 7. HashMap中put方法的过程/原理?

1. 调用`hash(key)`方法计算key的hash值, 然后结合数组长度, 计算数组下标; 具体算法就是把hash值与数组长度减一进行按位与运算, `hash & (length - 1)`;

2. 调整数组大小, 当容器中的元素个数大于`capacity * loadfactor`时, 默认是16 * 0.75, 容器会进行扩容调用`resize`方法, 扩容为原来的2倍;

3. 判断key的hash值在`HashMap`中是否存在:

    - 如果不存在则直接插入到数组下标位置;

    - 如果存在, 再判断key值是否相等, 也就是调用equals方法。 如果不存在, 则直接覆盖原value的值, 否则说明发生了哈希碰撞;

    - 发生哈希碰撞, 再判断相应位置的元素是否是`TreeNode`类型, 如果不是, 则表示当前是链表结构, 那就直接在链表的尾部插入元素, 如果链表的长度大于8时, 就将链表的结构树化; 如果当前位置元素类型是`TreeNode`, 则说明当前数据结构是红黑树, 那就将元素插入到红黑树中。

![collection_hashmap_put](/image/collection_hashmap_put.png)

### 8. HashMap扩容的过程?

1. 创建一个新的数组, 其容量是旧数组的2倍(`newCap = oldCap << 1`), 同时扩展长度也变为原来的2倍(`newThr = oldThr << 1`);

2. 当扩容大小确定时, 遍历原来的table, 将原table中的每个元素放入到新table中;

3. 如果元素Node的next节点为null, 也就是说不存在链表以及红黑树的情况下, 根据hash值重新计算新数组的下标位置, 放入到新table中。 其中`e.hash & (newCap - 1)`就是计算原来的元素e在新table中的位置;

4. 如果元素Node的类型是`TreeNode`, 就表示此时该位置上存在红黑树, 那就需要重新调整红黑树的结构, 如果树的size很小, 默认是6, 那就将红黑树退化成链表;

5. 如果该位置的数据结构是链表, 那就重新计算元素的位置下标。链表节点在新数组中的位置下标只有两种: 一种是原下标的位置, 一种是原下标加上旧数组的长度;

    > 通过将元素的hash值与旧数组的长度进行按位与`&`运算, 结果只有两种: 一种是等于0, 一种是大于0; <br>
    > 当结果等于0, 就直接使用原数组的下标; 当结果大于0, 就是原下标加上旧数组的长度的值, 作为新数组的下标。<br>
    > 因为数组的长度必须是2的n次幂减一, 在二进制中高位为0, 低位为1的表示, 这样跟元素的hash值进行与运算, 所得到的的结果, 就只有两种。要么等于0, 要么大于0。<br>

    假设table原长度是16, 扩容后长度就是32, 那么一个hash值在扩容前后的table下标计算如下:

    ![collection_hashmap_resize](/image/collection_hashmap_resize.png)

    从图中可以看到, 链表中元素的hash值, 与新旧table的长度进行按位与运算的结果, 最后四位显然是相同的, 唯一可能出现的区别的地方就在第5位, 也就是hash值b所在的位置, 如果b所在的位置是运算结果是0, 那么这个元素的位置与新table相同, 反之如果b所在的位置是1, 则新table重新计算的位置结果就比原来的位置多了10000(二进制), 而这个二进制10000的结果就是旧table的长度16。

    所以在代码中`e.hash & oldCap`, 链表元素的hash值与旧table的长度进行按位与`&`运算的结果。 如果最终结果是0, 则表示新下标就等于原下标, 如果不等于那么新下标等于旧下标的长度加上当前元素的位置。

### 9. 为什么不直接使用红黑树, 而是先使用链表?

1. 首先不选二叉查找树, 就是因为二叉查找树在特殊情况下, 会导致树的高度很深, 变成一条线性结构, 这就跟原来的链表一样了, 遍历查找非常影响效率;

2. 引入红黑树是为了解决链表过长, 导致查找元素时, 需要遍历链表, 效率很低的问题。

但是红黑树是平衡二叉树在插入新元素时, 可能需要通过左旋, 右旋, 变色的操作来保持平衡, 但是由于, 所以要保持平衡也是需要进行操作, 但是这些操作比遍历很长的链表效率要高。如果链表长度很短的话, 根本不需要引入红黑树, 引入反而会慢。

### 10. 说说你对红黑树的理解?

1. 每个节点非红即黑

2. 根节点总是黑色的

3. 如果节点是红色的，则它的子节点必须是黑色的(反之不一定)

4. 每个叶子节点都是黑色的空节点(NIL节点)

5. 从根节点到叶节点或空子节点的每条路径，必须包含相同数目的黑色节点(即相同的黑色高度)

### 11. HashMap, LinkedHashMap, TreeMap有什么区别?

- `HashMap`: 在Map中插入, 删除和定位元素时, 常用。

- `LinkedHashMap`: 当需要输出的顺序和输入顺序相同的情况下使用

- `TreeMap`: 当需要按照key的自然顺序或者自定义顺序遍历key的情况下使用; 自定义顺序就是实现`Comparable`接口

### 12. 构造HashMap时, 如果传入的初始容量值不是2的n次幂怎么办?

`HashMap`中对传入的初始化容量参数进行校验, 如果不是2的n次幂, 就通过一定的方法, 找到大于这个值的最近的2的n次幂。

具体的方法: 

因为我们发现`2^n-1`的值二进制, 低位都是1, 其他位都是0, 所以将原值通过无符号右移分别右移1次, 2次, 4次, 8次, 16次 并且 每次右移在与之前的值进行或运算`|`, 然后可以得到`2^n-1`的二进制, 最后再加1即可。

### 13. 为什么HashMap的容量一定是2的n次幂?(面试高频)

我认为最主要是有两点原因:

1. 使元素可以平均的分布在数组上, 减少hash碰撞, 避免形成链表;

    `HashMap`中的hash算法是:

    ```java
    hashcode & (length - 1)
    ```

    至于为什么使用位运算, 而不是取模运行。最主要的原因就是位运算的速度更快。

    其中length就是数组长度, hash就是插入值的hash值, 如果length=2^n的话, 那么length-1 = 2^n-1。

    我们通过上面的示例分析可以看到, 2^n-1用二进制表示, 所有低位都是1, 此时跟hash值进行`&`与运算, 可以保证结果均匀的散列在数组中, 减少hash碰撞。

    相反的, 如果数组长度不是2的n次幂, 则出现hash碰撞的可能性会大大提高。如下图示例:

    ![collection_hashmap_tablesize2n](/image/collection_hashmap_tablesize2n.png)

2. 当`HashMap`进行扩容操作时, 是否需要迁移元素的位置

    在扩容时, 通过元素的hash值与原数组的长度进行与运算, 来判断是否需要移动当前元素。`hash & oldTable.length`。 
    如果这个值为0, 则表示移动之后的位置就是现在的位置, 也就不需要移位了; 
    而如果大于0, 那这个值正好是原位置加上旧数组的长度, 作为新元素的下标。

    为什么可以这样扩容迁移数据, 就是因为数组的大小是2的n次幂, 而且扩容时新数组的大小就是旧数组的2倍。

### 14. 如果初始化HashMap时, 传入一个不是2的n次幂的值, 怎么办?

在`HashMap`内部对这个参数进行了校验, 如果初始化`HashMap`时, 初始容量不是2的n次幂, 会找到大于这个数并且最接近2的n次幂的那个数。

> 如何实现寻找最近2次幂的数?

通过对原来的数进行右移1位, 2位, 4位, 8位, 16位, 并且每次移动都与之前的进行或运算`|`, 那么最后一定可以得到高位都是0, 低位都是1的二进制数, 而这个数正式2的n次幂减一, 最后在对这个值加1就可以得到最近2次幂的数。

### 15. HashMap为什么用红黑树而不用AVL(平衡二叉树), 或者B+树?

AVL是一个高度平衡的二叉查找树, 它虽然保证了搜索, 插入, 删除操作平均的时间复杂度都是O(log n), 但是增加和删除可能需要通过一次或多次的旋转来重新平衡这个树。

但是红黑树并不追求完美, 它只要求部分平衡, 降低旋转的要求。

红黑树是牺牲了严格的高度平衡为代价, 来提升性能。

> 红黑树常常不会单独使用, 而是与数组关联。也就是类似`HashMap`的实现方式。`TreeMap`和`TreeSet`的实现底层都是直接通过红黑树算法, 也就意味着`TreeMap`添加元素, 取出元素的性能都比`HashMap`低。

`TreeMap`添加元素, 都需要通过循环找到新增的Entry插入位置, 因此比较消耗性能, 而取出元素, 也需要遍历树。

所以合理的使用红黑树可以提高性能, 反正可能回影响性能。

### 16. HashMap和HashTable有什么区别?

1. `HashMap`是线程不安全的, `HashTable`是线程安全的;
2. `HashMap`最多允许一条记录的键为null, 允许多条记录的值为null, 但是`HashTable`不允许;
3. `HashMap`默认初始化数组的大小为16, `HashTable`为11, 前者扩容时, 大小为原来的2倍, 后者扩大2倍加1;
4. `HashMap`需要重新计算hash值, 而`HashTable`直接使用对象的hashCode。

### 16. Java中另一个线程安全的与`HashMap`极其类似的类是什么? 同样是线程安全的, 它与`HashTable`在线程同步上有什么不同?

这个类是`ConcurrentHashMap`类。

在JDK1.7中采用分段锁的方式, JDK1.8中直接采用`CAS`, `自旋锁`, `volatile`, `synchronized`的方式。而`HashTable`是直接通过`synchronized`关键字加锁。

在并发程度上, `ConcurrentHashMap`的支持并发量更高。

### 18. 针对 ConcurrentHashMap 锁机制具体分析(JDK 1.7 VS JDK 1.8)?

JDK1.7中, 采用分段锁的机制, 实现并发的更新操作, 底层采用**数组+链表**的存储结构, 其中实现分段的核心是两个静态内部类`Segment`和`HashEntry`。

- `Segment`继承`ReentrantLock`来充当锁的角色, 每个`Segement`对象维护哈希数组的若干位置;

- `HashEntry`是用来封装哈希数组的键值对;

- 每个元素的位置(桶)是由若干个`HashEntry`对象连接起来的链表

![collection_hashmap_interview1](/image/collection_hashmap_interview1.webp)

JDK1.8中, 采用`CAS` + `synchronized`来保证并发安全, 取消类`Segment`, 直接使用数组中的元素作为锁对象, 分段锁的粒度更细, 当操作数组中不同元素时, 是不会产生竞争的, 也就是会并行处理。

### 19. HashMap 与 ConcurrentHashMap的区别?

除了`ConcurrentHashMap`支持并发处理元素之外, 原理上其实区别不大。

- `HashMap`的键值对允许有null, 而`ConcurrentHashMap`不允许;
- `HashMap`在遍历元素过程中, 如果插入元素或者删除元素, 会直接抛出异常; 而`ConcurrentHashMap`并不会, 它只会判断它的next元素是否为空。

### 20. 为什么ConcurrentHashMap 比 HashTable 效率要高?

`HashTable`使用的一把锁, 直接锁住了整个链表结构处理并发问题, 导致了多个线程竞争一把锁, 产生阻塞;

`ConcurrentHashMap`不管是JDK1.7还是JDK1.8采用的都是分段锁的思想, 每个线程操作的元素都是一把锁, 这样只要操作的不是用一个元素, 那么多个线程就会并行处理, 大大提高了效率。

### 21. ConcurrentHashMap在JDK1.8中, 为什么要使用内置锁synchronized来代替重入锁ReentrantLock?

1. JVM对`synchronized`的优化, 使得它的效率与`ReentrantLock`差不多;

2. 如果是在大量数据操作下, 对于JVM的内存压力, 基于API的`ReentrantLock`会开销更多的内存, 因为`ReentrantLock`是基于CAS的, 所以大量数据操作时, 可能存在大量线程一直在循环状态, 导致压力过大

### 22. ConcurrentHashMap 简单介绍?

1. `ConcurrentHashMap`是通过内部的一个`sizeCtl`变量来判断当前容器的状态:

    - 如果`sizeCtl < 0` 则表示, 正在初始化或者正在扩容; `sizeCtl`的高16位是扩容的标识戳, 低16位是表示有N-1个线程正在进行扩容;

    - 如果`sizeCtl = 0` 则表示, table还没有初始化;

    - 如果`sizeCtl > 0`, 则表示初始化完成, 并且作为下一次进行扩容的大小。

2. `Node`节点是基本存储结构, 实现了`Map`中的`Entry`, 用于存储数据, 其中包含元素的hash值, 键值对, 以及next节点:

    - TreeNode继承了Node, 是红黑树的基本存储结构

    - TreeBin是封装TreeNode的容器, 主要是提供转换红黑树的锁的控制

3. 添加元素时调用 `putVal`方法, 具体步骤如下:

    - 如果哈希表数组未初始化, 则初始化;

    - 如果新插入的元素下标所在位置为空, 则尝试把新元素直接插入到该数组位置;

    - 如果正在扩容, 则当前线程一起加入到扩容的过程中; 因为数组的扩容涉及到数据的迁移, 所以可以利用多个线程同时进行迁移工作; 在源码可以看到 `(fh = f.hash) == MOVED` 根据每个节点的状态可以判断, 当前容器是否正在扩容。

    - 如果新插入的元素所在下标的位置不为空, 并且此时也不再扩容, 那么则给这个下标位置加锁操作(也就是加上`synchronized`); 这里可以看到加锁的是每个数组中的元素, 也就是说只要多个线程操作的不是同一个元素, 那就是真正的并行去处理每个元素;

    - 如果当前下标位置中的元素存储结构是链表, 则遍历该链表, 判断这个key是否在链表中; 如果是, 则替换原值; 否则就在链表尾部插入元素;

    - 如果当前位置中的元素存储结构是红黑树, 也遍历这个树, 判断key是否在树中; 如果是, 则替换原值; 否则插入到红黑树中; 在插入红黑树后, 还需要判断是否调整红黑树;

    - 如果元素已经存在, 则返回旧值;

    - 如果元素不存在, 整个Map的元素个数加1, 并且检查是否需要扩容。

4. 判断是否需要扩容调用 `addCount`方法, 获取元素的个数采用的也是分段的思想, 计算元素个数时, 把这些段的值以及baseCount相加算出总的元素个数;

5. 扩容方法调用`transfer()`方法: 默认容量为16, 扩容时容量变为原来的两倍, 具体的扩容方式与`HashMap`基本相同, 要注意的是这里的扩容门槛就是`sizeCtl`相当于`threshold`, 它是一个固定的值 原来数组大小的0.75倍;

6. 获取元素调用`get`方法时, 先计算hash值, 定位到该数组的位置, 如果当前数组正在扩容, 那就标记正在扩容的节点, 这个节点是`ConcurrentHashMap`在扩容时用到的专门定义的内部类`ForwardingNode`。利用`ForwardingNode.find()`方法可以找到, 正在扩容时, 查找到该节点的元素

    > 要注意的是, 获取元素是不加锁的, 所以`ConcurrentHashMap`不是数据强一致性的, 如果一个线程在添加key元素, 另一个线程在获取这个key, 可能是无法获取到的。

### 23. ConcurrentHashMap 的并发度是什么?

我们知道`ConcurrentHashMap`在多线程同时更新元素而不产生锁竞争的最大线程数。默认是16, 并且可以在构造函数中设置。

当用户设置并发度时，ConcurrentHashMap 会使用大于等于该值的最小2幂指数作为实际并发度(假如用户设置并发度为17，实际并发度则为32)

