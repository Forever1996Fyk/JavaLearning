## <center>RocketMQ 相关问题</center>

### 1. 如何解决顺序消费和重复消费

**`RocketMQ`在主题上是无序的, 它只有在队列层面才是保证有序的。** 


#### 1.1 顺序消费

顺序消费中一般有两种模式: **普通顺序和严格顺序。**

- 普通顺序是指消费者通过**同一个消费队列收到的消息是有顺序的, 不同消息队列收到的消息可能是无顺序的。**
普通顺序消息在 **`Broker`重启情况下不会保证消息顺序性(短暂时间)。**

- 严格殊勋是指消费者收到的所有消息均是顺序的。**即使在异常情况下也会保证消息的顺序性。**

严格顺序模式看起来很好, 但是实现它会付出很大的代价。如果使用严格顺序模式, `Broker`集群中只要有一台机器不可用, 则整个集群都不可用。现在主要场景也就用在`binlog`同步。

一般情况下, 我们的MQ都是能容忍短暂的乱序, 所以推荐使用普通顺序模式。

现在问题来了, 如果我们采用了**普通顺序模式**, 我们之前已经知道了`Producer`生产消息时会进行轮询(取决于负载均衡策略)来向同一主题的不同消息队列发送消息。

> 如果此时有几个消息分别是同一个订单的创建, 支付, 发货, 在轮询的策略下这`三个消息会被发送到不同的队列`, 而在不同队列中就无法使用`RocketMQ`的保证消息的有序性, 怎么解决?

![RocketMQ_messageOrder](/distributed/MQ/img/RocketMQ_messageOrder.jpg)

其实很简单, 我们需要处理的仅仅是将同一语义下的消息放入同一个队列(比如这里的同一个订单), 那我们可以使用**Hash取模法**来保证同一个订单在同一个队列中就行了。

#### 1.2 重复消费

首先解决重复消费的办法就两个字 ———— **幂等**。**在编程中`幂等`操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。**

例如: 有一个订单的处理积分的系统, 每当来一个消息的时候它就负责为创建这个订单的用户积分加上相应的数值。现在消息队列发送给订单系统user1的订单消息, 要求是给user1的积分加上500。但是积分系统在收到user1的订单消息处理完成之后, 返回给消息队列处理成功的响应时出现了网络波动,或者`Broker`意外重启等等, 这条响应没有发送成功。

那么, 消息队列没有收到这个积分系统的响应会不会尝试重发这个消息?如果重新发送, 万一它又给user1加上500积分怎么办?

针对上面出现的情况就需要给消费者实现**幂等**, 也就是对同一个消息的处理结果, 执行多少次都不变。

实现**幂等**的方式需要根据具体的业务来决定的。

- 写入`Redis`来保证, 因为`Redis`的`key`和`value`就是天然支持幂等的;
- 数据库插入法, 基于数据库的唯一键来保证重复数据不会被插入多条。

在整个互联网领域, **幂等**不仅仅适用于消息队列的重复消费问题, 还有很多其他的场景来解决重复请求或者重复调用的问题。比如将HTTP服务设计成幂等的, **解决前端或者APP重复提交表单数据的问题**, 也可以将一个微服务设计成幂等的, 解决`RPC`框架自动重试导致的**重复调用问题。**

### 2. 消息堆积问题

我们之前学习了为什么要使用消息队列, 其中一个很重要的功能————**削峰**。但是如果这个峰值太大了导致消息堆积在队列中怎么办?

其实这个问题可以将它广义化, **因为生产消息堆积的根源只有两个————生产者生产太快或者消费者消费太慢**。

所以解决这个问题可以从多个角度去思考, 当流量到峰值的时候是因为生产者生产太快, 我们可以使用一些**限流降级**的方法, 当然也可以增加多个消费者实例去水平扩展增加消费能力来匹配生产的激增。

如果消费者消费过慢的话, 我们可以先检查**是否是消费者出现了大量的消费错误**, 或者打印日志查看是否哪一个线程卡死, 出现了锁资源不释放等问题。

> 最快解决消息堆积问题的方法还是增加消费者实例, 不过你还需要同时增加每个主题的队列数量。因为在`RocketMQ`中, **一个队列只会被一个消费者消费**, 如果仅仅增加哦消费者实例就会出现消费位移错误的情况。

### 3. 回溯消费

回溯消费是指`Consumer`已经消费成功的消息, 由于业务上需求需要重新消费。在`RockeetMQ`中, `Broker`在向`Consumer`投递成功消息后, **消息仍然需要保留。**并且重新消费一般是按照时间维度, 例如: 由于`Consumer`系统故障, 恢复后需要重新消费1小时前的数据, 那么`Broker`要提供一种机制, 可以按照时间维度来回退消费进度。`RocketMQ`支持按照时间回溯消费, 时间维度精确到毫秒。

### 4. RocketMQ的刷盘机制

#### 4.1 同步刷盘和异步刷盘

![rocket_flush_disk](/distributed/MQ/img/rocket_flush_disk.jpg)

根据图示, 在同步刷盘中需要等待一个刷盘成功的`ACK`, 同步刷盘对`MQ`的消息可靠性来说是很好的保障, 但是**性能上会有较大的影响**, 一般适用于金融等特定业务场景。

异步刷盘往往是开启一个线程去异步的执行刷盘操作。消息刷盘采用后台异步线程提交的方式进行, **降低了读写延迟**, 提高了`MQ`的性能和吞吐量, 一般适用于发送验证码等对于消息保证要求不太高的业务场景。

在一般情况下, **异步刷盘只有在`Broker`意外宕机的时候会丢失部分数据**, 你可以设置`Broker`的参数`FlushDiskType`来调整你的刷盘策略(ASYNC_FLUSH或者SYNC_FLUSH)。

#### 4.2 同步复制和异步复制

上面的同步刷盘和异步刷盘实在单个节点层面的, 而同步复制和异步复制主要是在`Broker`主从模式下, 主节点返回消息给客户端的时候是否需要同步从节点。

- 同步复制: 也叫"同步双写", 也就是说, **只有消息同步双写到主从节点上时才返回写入成功。**
- 异步复制: **消息写入主节点之后就直接返回写入成功。**

> 异步复制会不会也像异步刷盘那样影响消息的可靠性呢?

答案是不会的, 因为两者就是不同的概念, 对于消息可靠性是通过不同的刷盘策略保证的, 而像异步同步复制策略仅仅是影响到**可用性**。
其最主要原因是 **`RocketMQ`是不支持自动主从切换的, 当主节点挂掉之后, 生产者就不能再给这个主节点生产消息了**。

例如: 采用异步复制的方式, 在主节点还未发送完需要同步的消息的时候主节点宕机了, 这个时候从节点就少了一部分消息。但是此时生产者无法再给主节点生产消息了, **消费者可以自动切换到从节点进行消费(仅仅是消费)**, 所以在主节点宕机的时间内只会产生主从节点短暂的消息不一致的情况, 降低了可用性, 而当主节点重启之后, 从节点丢失的消息还会继续复制。

在单一的主从架构中, 如果主节点挂掉了, 那也就意味着整个系统不能再生产了, 那么可用性如何解决? **一个主从不行那就多个主从。** 每个`Topic`是分布在不同的`Broker`中的。

但是这样也会带来一个问题, 那就是无法保证**严格顺序**。如果此时我们主节点A负责的是订单A的一系列消息, 然后它宕机了, 这样其他节点是无法代替主节点A的, 如果我们任意节点都可以存入任何消息, 那就没有顺序性可言了。

在`RocketMQ`中采用了`Dledger`解决这个问题。它要求在写入消息的时候, 要求**至少消息复制到半数以上的节点之后**, 才给客户端返回写入成功, 并且它是支持通过选举来动态切换主节点的。

### 5. 存储机制

> 在`Topic`中**队列是以什么样的形式存在的? 队列中的消息有事如何进行存储持久化的呢?**

`RocketMQ`消息存储架构中的三大角色-- `CommitLog`, `ConsumeQueue`, `IndesFile`。

- `CommitLog`: **消息主体以及元数据的存储主体**, 存储`Producer`端写入消息主体内容, 消息内容不是固定的。单个文件大小默认1G, 文件名长度为20位(为什么要设计成固定的? 内存映射机制), 左边补零, 剩余为起始偏移量, 比如00000000000000000000代表第一个文件, 起始偏移量为0, 文件大小为1G=1073741824；当第一个文件写满了，第二个文件为00000000001073741824，起始偏移量为1073741824，以此类推。 消息主要是**顺序写入日志文件**, 当文件满了, 写入下一个文件。

- `ConsumeQueue`: 消息消费队列, 引入的目的主要是**提高消息的消费性能**, 由于`RocketMQ`是基于主题的`Topic`的订阅模式, 消息消费是针对主题进行的, 如果要遍历`CommitLog`文件中根据`Topic`检索消息是非常低效的。`Consumer`即可根据`ConsumeQueue`来查找待消费的消息。**`ConsumeQueue`作为消费消息的索引, 保存了指定`Topic`下的队列消息在`CommitLog`中的起始物理偏移量`offset`, 消息大小`size`和消息`Tag`的`HashCode`值。**

- `IndexFile`: `IndexFile`(索引文件)提供了一种可以通过key或时间区间来查询消息的方法。

`RocketMQ`采用的是**混合型的存储结构**, `Broker`单个实例下所有的队列共用一个日志数据文件在存储消息。因为这个可以**提高数据的写入效率**, 不分`Topic`意味着我们有更大的几率获取**成批**的消息进行数据写入, 但是也会导致读取消息的时候需要遍历整个大文件十分耗时的问题。

所以在`RocketMQ`中又使用`ConsumeQueue`作为每个队列的索引文件来**提升读取消息的效率**。我们可以直接根据队列的消息序号, 计算出索引的全局位置(索引序号*索引固定长度20), 然后直接读取这条索引, 再根据索引中记录的消息全局位置找到消息。

![rocket_message_storage](distributed/MQ/img/rocket_message_storage.jpg)

根据上图, 基本上就知道了`RocketMQ`的存储消费过程了。

生成者发送消息会指定`Topic`, `QueueId`和具体消息内容, 而在`Broker`中不管什么消息, 他直接全部存储到`CommitLog`中, 而根据生成者指定的`Topic`和`QueueId`将这条消息本身在`CommitLog`的偏移(offset), 消息本身大小, 和`tag`的hash值存入对应的`ConsumeQueue`索引文件中。

在每个队列中都保存了`ConsumeOffset`, 即每个消费者组的消费位置, 而消费者拉取消息进行消费的时候只需要根据`ConsumeOffset`获取下一个未被消费消息即可。